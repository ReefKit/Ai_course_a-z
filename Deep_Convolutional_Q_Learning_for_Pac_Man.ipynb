{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAiHVEoWHy_D"
   },
   "source": [
    "# Deep Convolutional Q-Learning for Pac-Man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjO1aK3Ddjs5"
   },
   "source": [
    "## Part 0 - Installing the required packages and importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwdRB-ZLdrAV"
   },
   "source": [
    "### Installing Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbnq3XpoKa_7",
    "outputId": "13b0f2f2-34f2-4f4a-f6e4-3419355173f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (1.0.0)\n",
      "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: ale-py>=0.9 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[accept-rom-license,atari]) (0.10.1)\n",
      "Requirement already satisfied: ale-py in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>1.20 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from ale-py) (2.2.1)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Requirement already satisfied: gymnasium[box2d] in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (2.2.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages (from gymnasium[box2d]) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!pip install ale-py\n",
    "!apt-get install -y swig\n",
    "!pip install gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-wes4LZdxdd"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ho_25-9_9qnu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7wa0ft8e3M_"
   },
   "source": [
    "## Part 1 - Building the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlYVpVdHe-i6"
   },
   "source": [
    "### Creating the architecture of the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, action_size, seed = 42):\n",
    "        super(Network, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size = 8, stride = 4) # convolution\n",
    "        self.bn1 = nn.BatchNorm2d(32) # batch normalization operation\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 4, stride = 2) # convolution\n",
    "        self.bn2 = nn.BatchNorm2d(64) # batch normalization operation\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size = 3, stride = 1) # convolution\n",
    "        self.bn3 = nn.BatchNorm2d(64) # batch normalization operation\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1) # convolution\n",
    "        self.bn4 = nn.BatchNorm2d(128) # batch normalization operation\n",
    "        self.fc1 = nn.Linear(10 * 10 * 128, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.bn1(self.conv1(state)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUvCfE_mhwo2"
   },
   "source": [
    "## Part 2 - Training the AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWCDPF22lkwc"
   },
   "source": [
    "### Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dx8WQ7lbeY_z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (210, 160, 3)\n",
      "State size:  210\n",
      "Number of actions:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reefk/miniforge3/envs/ml-gpu-env/lib/python3.12/site-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n",
    "state_shape = env.observation_space.shape\n",
    "state_size = env.observation_space.shape[0]\n",
    "number_actions = env.action_space.n\n",
    "print('State shape: ', state_shape)\n",
    "print('State size: ', state_size)\n",
    "print('Number of actions: ', number_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bx6IdX3ciDqH"
   },
   "source": [
    "### Initializing the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-4\n",
    "minibatch_size = 64\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2bDShIEkA5V"
   },
   "source": [
    "### Preprocessing the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def preprocess_frame(frame): # build tensor from frame\n",
    "    frame = Image.fromarray(frame) # frame is now a PIL image object\n",
    "    preprocess = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()]) # resize for neurel network, then convert PIL image to pytorch tensor\n",
    "    return preprocess(frame).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imMdSO-HAWra"
   },
   "source": [
    "### Implementing the DCQN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "  def __init__(self, action_size):\n",
    "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    self.action_size = action_size\n",
    "    self.local_qnetwork = Network(action_size).to(self.device)\n",
    "    self.target_qnetwork = Network(action_size).to(self.device)\n",
    "    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr = learning_rate)\n",
    "    self.memory = deque(maxlen = 10000)\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    state = preprocess_frame(state) # numpy array -> pytorch tensor\n",
    "    next_state = preprocess_frame(next_state)\n",
    "    self.memory.append((state, action, reward, next_state, done))\n",
    "    if len(self.memory) > minibatch_size:\n",
    "        experiences = random.sample(self.memory, k = minibatch_size)\n",
    "        self.learn(experiences, discount_factor)\n",
    "\n",
    "  def act(self, state, epsilon = 0.):\n",
    "    state = preprocess_frame(state).to(self.device) # input image -> input vector\n",
    "    self.local_qnetwork.eval()\n",
    "    with torch.no_grad():\n",
    "      action_values = self.local_qnetwork(state)\n",
    "    self.local_qnetwork.train()\n",
    "    if random.random() > epsilon:\n",
    "      return np.argmax(action_values.cpu().data.numpy())\n",
    "    else:\n",
    "      return random.choice(np.arange(self.action_size))\n",
    "\n",
    "  def learn(self, experiences, discount_factor):\n",
    "    states, actions, rewards, next_states, dones = zip(*experiences) # these are already pytorch tensors\n",
    "    states = torch.from_numpy(np.vstack(\n",
    "      states\n",
    "      )).float().to(self.device)\n",
    "    actions = torch.from_numpy(np.vstack(\n",
    "      actions\n",
    "      )).long().to(self.device)\n",
    "    rewards = torch.from_numpy(np.vstack(\n",
    "      rewards\n",
    "      )).float().to(self.device)\n",
    "    next_states = torch.from_numpy(np.vstack(\n",
    "      next_states\n",
    "      )).float().to(self.device)\n",
    "    dones = torch.from_numpy(np.vstack(\n",
    "      dones\n",
    "      ).astype(np.uint8)).float().to(self.device)\n",
    "    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    q_targets = rewards + (discount_factor * next_q_targets * (1 - dones))\n",
    "    q_expected = self.local_qnetwork(states).gather(1,actions)\n",
    "    loss = F.mse_loss(q_expected, q_targets)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUg95iBpAwII"
   },
   "source": [
    "### Initializing the DCQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(number_actions)\n",
    "print(torch.cuda.get_device_name(agent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CK6Zt_gNmHvm"
   },
   "source": [
    "### Training the DCQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 290.70\n",
      "Episode 200\tAverage Score: 406.70\n",
      "Episode 300\tAverage Score: 445.00\n",
      "Episode 400\tAverage Score: 474.00\n",
      "Episode 500\tAverage Score: 477.70\n",
      "Episode 504\tAverage Score: 500.70\n",
      "Environment solved in 404 episodes!\tAverage Score: 500.70\n"
     ]
    }
   ],
   "source": [
    "number_episodes = 2000\n",
    "maximum_number_timestep_per_episode = 10000\n",
    "epsilon_starting_value = 1.0\n",
    "epsilon_ending_value = 0.01\n",
    "epsilon_decay_value = 0.995\n",
    "epsilon = epsilon_starting_value\n",
    "scores_on_100_episodes = deque(maxlen = 100)\n",
    "\n",
    "for episode in range(1, number_episodes + 1):\n",
    "  state, _ = env.reset()\n",
    "  score = 0\n",
    "  for t in range(maximum_number_timestep_per_episode):\n",
    "    action = agent.act(state, epsilon)\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    agent.step(state, action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    score += reward\n",
    "    if done:\n",
    "      break\n",
    "  scores_on_100_episodes.append(score)\n",
    "  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n",
    "  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end = \"\")\n",
    "  if episode % 100 == 0:\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n",
    "  if np.mean(scores_on_100_episodes) >= 500.0:\n",
    "    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n",
    "    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0WhhBV8nQdf"
   },
   "source": [
    "## Part 3 - Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cb9nVvU2Okhk"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     22\u001b[0m     imageio\u001b[38;5;241m.\u001b[39mmimsave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpacman.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m show_video_of_model(\u001b[43magent\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMsPacmanDeterministic-v0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_video\u001b[39m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpacman.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     27\u001b[0m     mp4list \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "import os\n",
    "\n",
    "# Set the path to FFmpeg explicitly\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/home/reefk/miniforge3/envs/ml-gpu-env/bin/ffmpeg\"\n",
    "\n",
    "def show_video_of_model(agent, env_name, filename='pacman.mp4'):\n",
    "    \"\"\"\n",
    "    Records a video of the agent interacting with the specified environment.\n",
    "\n",
    "    Args:\n",
    "        agent (object): The agent to evaluate.\n",
    "        env_name (str): Name of the Gym environment to use.\n",
    "        filename (str): Name of the output video file. Defaults to 'pacman.mp4'.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array')\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "    env.close()\n",
    "    imageio.mimsave(filename, frames, fps=30)  # Save the video to the specified filename\n",
    "\n",
    "def show_video(filename='pacman.mp4'):\n",
    "    \"\"\"\n",
    "    Displays a video saved as an MP4 file in the Jupyter Notebook.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Name of the video file to display.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):  # Check if the file exists\n",
    "        with open(filename, 'r+b') as video_file:\n",
    "            video = video_file.read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display(HTML(data=f'''\n",
    "            <video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{encoded.decode('ascii')}\" type=\"video/mp4\" />\n",
    "            </video>\n",
    "        '''))\n",
    "    else:\n",
    "        print(f\"Could not find video: {filename}\")\n",
    "\n",
    "# Record a video for the Ms. Pac-Man environment\n",
    "show_video_of_model(agent, 'MsPacmanDeterministic-v0', filename='pacman.mp4')\n",
    "\n",
    "# Display the recorded video\n",
    "show_video(filename='pacman.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
